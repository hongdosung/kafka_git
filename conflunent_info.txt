Confluent 란?
실시간 데이터 파이프라인 및 스티리밍 애플리케이션을 구축하는 데 사용되는 분산 이벤트 스트리밍 플랫폼.
실시간 분석, 데이터 수집, 이벤트 기반 아키텍쳐와 같은 사례에 적합


[용어 정리]
- 클러스터 외부
Connect : Connector를 동작하게 하는 프로세서 (서버)
Connector : Data를 어디에서(source) 어디로(sink) 복사할지 관리, 데이터를 처리하는 코드가 담긴 jar 패키지 
 ㄴ converter 설정을 통해 커넥터와 브로커 사이에 주고 받는 메시지를 어떻게 변환하여 저장할 것인지 설정 
Source Connector : Data Source의 데이터를 카프카 토픽에 보내는 역할을 하는 커넥터 (Producer)
Sink Connector : 카프카 토픽에 담긴 데이터를 특정 Data Source로 보내는 역할을 하는 커넥터 (Consumer)
Schema Registry : Data의 스키마 등록 및 조회, 버전관리, 호환성 체크등의 기능을 제공 
Ksql DB : 실시간 스트리밍 데이터를 처리, 내부 토픽을 Sub하여 분석하거나 Sub한 토픽을 정제하여 다시 토픽으로 Pub 가능  
Transforms : 메시지를 간단하게 수정할 수 있게 하는 tool (다만 다소 복잡한 변환을 수행해야 할 경우에는 KsqlDB나 Kafka Streams 사용을 권함.)
하나의 데이터를 받아 데이터를 수정해서 내보낸다.
여러개의 Transforms을 연결(chain)해서 하나의 connector에서 설정이 가능 

- 클러스터 내부
Broker : 각 kafka 클러스터의 노드(서버, 부트스트랩 서버),  n개의 topic으로 구성된다
부트스트랩 서버 : kafka client (producer, consumer 등)이 브로커와 연결하여 브로커 내부의 자원에 접근할 때 원하는 자원의 위치 (어떤 브로커에 해당 토픽이 저장되어 있는지)를 알기위한 메타데이터 공유.
Topic : kafka의 가장 기본적인 조직단위, 이벤트 스트림 구성 및 저장
새로운 이벤트 메시지가 topic에 기록되면 해당 메시지가 로그 끝에 추가된다.
이벤트가 기록된 후에는 변경 불가능
consumer가 각 topic을 읽으며 offset을 기록하여 순차적으로 log를 읽는다.
해당 topic을 구독하는 consumer는 0~n 개가 될 수 있다.
이벤트를 전송한 후에도 일정 기간동안 삭제되지 않는다.


=====================================================================
curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
        "name": "jdbc_source_mysql_02",
        "config": {
                "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                "connection.url": "jdbc:mysql://mysql:3306/demo",
                "connection.user": "connect_user",
                "connection.password": "asgard",
                "topic.prefix": "mysql-02-",
                "mode":"bulk",
                "poll.interval.ms" : 3600000
                }
        }'

curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
        "name": "jdbc_source_altibase_03",
        "config": {
                "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                "connection.url": "jdbc:mysql://mysql:3306/demo",
                "connection.user": "connect_user",
                "connection.password": "asgard",
                "topic.prefix": "mysql-03-",
                "mode":"bulk",
                "poll.interval.ms" : 3600000,
                "catalog.pattern" : "demo"
                }
        }'

속성의 의미는 다음과 같습니다.
name : source connector 이름(JdbcSourceConnector를 사용)
config.connector.class : 커넥터 종류(JdbcSourceConnector 사용)
config.connection.url : jdbc이므로 DB의 connection url 정보 입력
config.connection.user : DB 유저 정보
config.connection.password : DB 패스워드
config.mode : "테이블에 데이터가 추가됐을 때 데이터를 polling 하는 방식"(bulk, incrementing, timestamp, timestamp+incrementing)
config.incrementing.column.name : incrementing mode일 때 자동 증가 column 이름
poll.interval.ms: 소스 테이블을 복제하는 polling하고 복제하는 간격
catalog.pattern : kafka 토픽에 저장될 이름 pattern 지정

[상태 확인하기]
curl -X GET http://localhost:8083/connectors/jdbc_source_altibase_01/status
{"name":"jdbc_source_altibase_01","connector":{"state":"RUNNING","worker_id":"127.0.1.1:8083"}

[삭제하기]
curl -X DELETE http://localhost:8083/connectors/jdbc_source_altibase_01ㅎ